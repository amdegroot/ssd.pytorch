{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from data import VOCroot\n",
    "from data import VOC_CLASSES as labelmap\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "from data import AnnotationTransform, VOCDetection, base_transform\n",
    "from timeit import default_timer as timer\n",
    "import argparse\n",
    "import numpy as np\n",
    "from ssd import build_ssd\n",
    "import pickle\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.box_utils import jaccard, point_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def voc_ap(rec, prec):\n",
    "    \"\"\"VOC_AP average precision calculations using 11-recall-point based AP\n",
    "    metric (VOC2007)\n",
    "    [precision integrated to recall]\n",
    "    Params:\n",
    "        rec (FloatTensor): recall cumsum\n",
    "        prec (FloatTensor): precision cumsum\n",
    "    Return:\n",
    "        average precision (float)\n",
    "    \"\"\"\n",
    "    ap = 0.\n",
    "    for threshold in torch.range(0., 1., 0.1):\n",
    "        if torch.sum(rec >= threshold) == 0:  # if no recs are >= this thresh\n",
    "            p = 0\n",
    "        else:\n",
    "            # largest prec where rec >= thresh\n",
    "            p = torch.max(prec[rec >= threshold])\n",
    "        ap += p / 11.\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_net(net, cuda, valset, transform, top_k):\n",
    "    # dump predictions and assoc. ground truth to text file for now\n",
    "    num_images = len(valset)\n",
    "    ovthresh = 0.5\n",
    "    num_classes = 0\n",
    "\n",
    "    # per class\n",
    "    fp = defaultdict(list)\n",
    "    tp = defaultdict(list)\n",
    "    gts = defaultdict(list)\n",
    "    precision = Counter()\n",
    "    recall = Counter()\n",
    "    ap = Counter()\n",
    "\n",
    "    for i in range(5):\n",
    "        confidence_threshold = 0.01\n",
    "        print('Evaluating image {:d}/{:d}....'.format(i + 1, num_images))\n",
    "        img = valset.pull_image(i)\n",
    "        anno = valset.pull_anno(i)\n",
    "        # print(anno)\n",
    "        anno = torch.Tensor(anno).long()\n",
    "        gt_classes = list(set(anno[:, 4]))\n",
    "        x = Variable(transform(img).unsqueeze_(0))\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        y = net(x)  # forward pass\n",
    "        detections = y.data\n",
    "        # scale each detection back up to the image\n",
    "        scale = torch.Tensor([img.size[0], img.size[1],\n",
    "                              img.size[0], img.size[1]])\n",
    "        # for each class\n",
    "        if num_classes == 0:\n",
    "            num_classes = detections.size(1)\n",
    "        for cl in range(detections.size(1)):\n",
    "            dets = detections[0, cl, :, :]\n",
    "            mask = dets[:, 0].ge(0.01).expand(5, dets.size(0)).t()\n",
    "            # all dets w > 0.01 conf for class\n",
    "            dets = torch.masked_select(dets, mask).view(-1, 5)\n",
    "            mask = anno[:, 4].eq(cl).expand(5, anno.size(0)).t()\n",
    "            # all gts for class\n",
    "            truths = torch.masked_select(anno, mask).view(-1, 5)\n",
    "            if truths.numel() > 0:\n",
    "                truths = truths[:, :-1]\n",
    "                # gts[cl].extend([1] * truths.size(0))  # count gts\n",
    "                if dets.numel() < 1:\n",
    "                    continue  # no detections to count\n",
    "                # there exist gt of this class in the image\n",
    "                # check for tp & fp\n",
    "                preds = dets[:, 1:]\n",
    "                preds *= scale.unsqueeze(0).expand_as(preds)\n",
    "                # compute overlaps\n",
    "                overlaps = jaccard(truths.float(), preds)\n",
    "                # if each gt obj is found yet\n",
    "                found = [False] * overlaps.size(0)\n",
    "                maxes = overlaps.max(0)\n",
    "                for pb in range(overlaps.size(1)):\n",
    "                    max_overlap = maxes[0][0, pb]\n",
    "                    gt = maxes[1][0, pb]\n",
    "                    if max_overlap > ovthresh:  # 0.5\n",
    "                        if found[gt]:\n",
    "                            # duplicate\n",
    "                            fp[cl].append(1)\n",
    "                            tp[cl].append(0)\n",
    "                            gts[cl].append(0) # tp\n",
    "                        else:\n",
    "                            # not yet found\n",
    "                            tp[cl].append(1)\n",
    "                            fp[cl].append(0)\n",
    "                            found[gt] = True  # mark gt as found\n",
    "                            gts[cl].append(1) # tp\n",
    "                    else:\n",
    "                        fp[cl].append(1)\n",
    "                        tp[cl].append(0)\n",
    "                        gts[cl].append(0) # tp\n",
    "            else:\n",
    "                # there are no gts of this class in the image\n",
    "                # all dets > 0.01 are fp\n",
    "                if dets.numel() > 0:\n",
    "                    fp[cl].extend([1] * dets.size(0))\n",
    "                    tp[cl].extend([0] * dets.size(0))\n",
    "                    gts[cl].extend([0] * dets.size(0))  # fn\n",
    "    for cl in range(num_classes):\n",
    "        if len(gts[cl]) < 1:\n",
    "            continue\n",
    "        # for each class calc rec, prec, ap\n",
    "        tp_cumsum = torch.cumsum(torch.Tensor(tp[cl]), 0)\n",
    "        fp_cumsum = torch.cumsum(torch.Tensor(fp[cl]), 0)\n",
    "        gt_cumsum = torch.cumsum(torch.Tensor(gts[cl]), 0)\n",
    "        pos_det = max(tp_cumsum) + max(fp_cumsum)\n",
    "        # precision (tp / tp+fp)\n",
    "        # recall (tp+fp / #gt) => gt = tp + fn\n",
    "        # avoid div by 0 with .clamp(min=1e-12)\n",
    "        rec = tp_cumsum / gt_cumsum.clamp(min=1e-12)\n",
    "        prec = tp_cumsum / (tp_cumsum + fp_cumsum).clamp(min=1e-12)\n",
    "        ap[cl] = voc_ap(rec, prec)\n",
    "        recall[cl] = max(rec)\n",
    "        precision[cl] = max(prec)\n",
    "        print('class', cl, 'rec', recall[cl],\n",
    "              'prec', precision[cl], 'AP', ap[cl],\n",
    "              'tp', sum(tp[cl]), 'fp', sum(fp[cl]), 'gt', sum(gts[cl]))\n",
    "    # mAP = mean of APs for all classes\n",
    "    mAP = sum(ap.values()) / len(ap)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model!\n",
      "Evaluating image 1/2510....\n",
      "Evaluating image 2/2510....\n",
      "Evaluating image 3/2510....\n",
      "Evaluating image 4/2510....\n",
      "Evaluating image 5/2510....\n",
      "class 1 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 2 gt 0\n",
      "class 2 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 2 gt 0\n",
      "class 3 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 5 gt 0\n",
      "class 4 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 2 gt 0\n",
      "class 5 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 4 gt 0\n",
      "class 6 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 2 gt 0\n",
      "class 7 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 3 gt 0\n",
      "class 8 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 1 gt 0\n",
      "class 9 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 4 gt 0\n",
      "class 10 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 1 gt 0\n",
      "class 11 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 1 gt 0\n",
      "class 12 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 2 gt 0\n",
      "class 13 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 1 gt 0\n",
      "class 15 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 4 gt 0\n",
      "class 16 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 4 gt 0\n",
      "class 18 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 1 gt 0\n",
      "class 19 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 1 gt 0\n",
      "class 20 rec 0.0 prec 0.0 AP 0.0 tp 0 fp 1 gt 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = 'weights/ssd_300_VOC0712.pth'\n",
    "\n",
    "# load net\n",
    "net = build_ssd('test', 300, 21)    # initialize SSD\n",
    "net.load_state_dict(torch.load(trained_model))\n",
    "net.eval()\n",
    "print('Finished loading model!')\n",
    "# load data\n",
    "valset = VOCDetection(VOCroot, 'val', None, AnnotationTransform())\n",
    "# evaluation\n",
    "test_net(net, False, valset, base_transform(net.size,(104,117,123)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = defaultdict(list)\n",
    "d[1].extend([1] * 4)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
